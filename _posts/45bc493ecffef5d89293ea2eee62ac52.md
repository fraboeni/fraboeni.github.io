---
title: "All you need is Matplotlib"
date: 2022-04-13
excerpt: "In this blogpost I introduce the concept of differential privacy and show you how it can be applied to perform privacy-preserving data analysis."
permalink: /posts/2022/04/45bc493ecffef5d89293ea2eee62ac52/
image: 2021-03-15-differential-privacy.png
canonical_url: 'https://blog.franziska-boenisch.de'
header:
  teaser: 2021-03-15-differential-privacy.png
tags:
  - differential privacy
  - privacy
---

<script src="//yihui.org/js/math-code.js"></script>
<!-- Just one possible MathJax CDN below. You may use others. -->
<script async
  src="//mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

{% include base_path %}

## Federated Learning with Untrusted Servers is Not Private

*by Franziska Boenisch, Adam Dziedzic, Roei Schuster, Ali Shahin Shamsabadi, Ilia Shumailov, and Nicolas Papernot*
Every day, we all generate large amounts of data such as the text we type or the pictures we take. This data is useful: by training machine learning (ML) models on data collected from millions of individuals, a myriad of applications become possible, a prominent example of which is text auto-completion by our smartphone’s keyboard.

However, how can we collect such large amounts of data from so many users at one central server, given the communication overhead and the central storage that would be needed? And even if we could overcome the onerous communication and storage requirements, do we really *want* to transmit all this highly personal data to a central party such as our smartphone vendor? 

In a widely celebrated paper from 2017, a framework called *Federated Learning* was proposed to answer these concerns. In Federated Learning, training is performed locally on device using the individual user’s data. The products of this local training are then sent to a server, where they are aggregated from multiple devices into a global model. The server never receives the raw user data, only the products of the local training procedure (let’s call them *gradients* or *model updates*).





Since users’ data never leaves devices, FL has, for a long time, been promoted as “privacy-preserving ML”. Today --- thanks to a long line of research --- we know that model updates actually contain ample information on individuals’ data and allow full reconstruction of their data in some cases, meaning that FL cannot actually be considered privacy-preserving. However, *this did not prevent large companies from still promoting FL as privacy preserving, and deploying it to learn from millions of users’ sensitive data*. They probably did so due to two widely-adopted beliefs: first, that reconstruction methods are computationally costly, and typically obtain low-fidelity reconstructions, especially for data that is high dimensional, or containing multiple instances from the same class, or when the local gradients are calculated over many user data points. Second, that FL combines well with protocols that make reconstruction attacks significantly harder or even provably impossible, namely, (local) Differential Privacy and Secure Aggregation.

[Our new work](https://arxiv.org/pdf/2112.02918.pdf) questions both of these widely-held beliefs. Our main message is that, as of yet, FL provides no viable way to defend against a non-honest (“malicious”) server.  This means a server that intentionally deviates from the FL protocol as prescribed, in order to perform the attack. In other words, to get privacy, we still must trust the party that is responsible for FL deployment (e.g. our smartphone vendor).

We first show an attack where the central server actively manipulates the initial weights of the shared model, to directly extract user data perfectly, i.e. with zero error to the original data. Our attack is the first privacy attack in FL that relies on  manipulations of the shared model. Notably, we find that, depending on the deployment of the FL setup, our extraction procedure works even in the honest setting without  the server maliciously initializing the weights. Even in this setting, the server can exactly reconstruct user data instances in a matter of milliseconds, defeating the state-of-the-art in passive reconstruction attacks by orders of magnitude. To illustrate this, here are pictures our attacker extracted, armed with only a Python shell and Matplotlib.
