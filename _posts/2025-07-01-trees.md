---
title: "Feature Importance in Decision Trees"
date: 2025-07-01
excerpt: "In this blogpost I introduce the concept of decision trees, feature importance, and how these are useful for explainable AI."
permalink: /posts/2025/07/trees/
image: 9999-03-15-differential-privacy.png
canonical_url: 'https://blog.franziska-boenisch.de'
robots: noindex
hidden: true
header:
  teaser: trees.png
tags:
  - decision trees
  - feature importance
  - explainable AI
---
<script src="//yihui.org/js/math-code.js"></script>
<!-- Just one possible MathJax CDN below. You may use others. -->
<script async
  src="//mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

{% include base_path %}



This page provides study materials on decision trees. It introduces the intuition behind how decision trees work and explains how to build them using impurity-based importance calculations. The content also covers feature importance and discusses how these concepts relate to explainable AI.

## Watch the Talk

<iframe width="560" height="315" src="https://youtu.be/9C5EmAVkgAg" frameborder="0" allowfullscreen></iframe>

---

## Download the Slides

[![Slides Preview](/images/trees.png)](https://yourdomain.com/path-to-your-slides.pdf)

<a href="{% include base_path %}/assets/slides/decision_trees.pdf">
  <img src="/images/trees.png" alt="Slides Preview" width="250">
</a>

> ðŸ“„ Click the image to download/view the slides (PDF).

---

## Play with the Code  

The following code shows you how to use sklearn's decision trees. If you want to practice how to implement a decision tree, please check out the coding exercise I prepared in this [GitHub Repo](https://github.com/yourusername/your-repo)
```python
import pandas as pd             # to have nice data frames
from sklearn import tree        # for the decision tree
import matplotlib.pyplot as plt # for plotting
import numpy as np

data = pd.read_csv("dataset_tml.csv",index_col=0)

# Map categorical features and label
data = data.replace({"Yes": 1, "No": 0}) # yes and no strings are mapped to 1 and 0

# Split into features and label
X = data.iloc[:, :-1]  # all columns except "Passed"
Y = data.iloc[:, -1]   # the "Passed" column

clf_gini = clf_gini.fit(X, Y)
clf_gini.predict(X)

# Look into the feature importance
impotances_gini = clf_gini.feature_importances_
print(impotances_gini)
```

## Find Additional Study Materials:

1. Full [Stanford Lecture](https://www.youtube.com/watch?v=wr9gUr-eWdA) on Decision Trees
2. Small [Lecture Video](https://www.youtube.com/watch?v=_L39rN6gz7Y ) on Gini Impurity
3. Book: Pattern Recognition and Machine Learning, [Chapter 14.4](https://github.com/Benlau93/Data-Science-Curriculum/blob/master/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf)
