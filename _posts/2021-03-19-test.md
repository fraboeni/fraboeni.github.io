---
title: "All You Need Is Matplotlib "
date: 2022-04-13
excerpt: "In this blogpost I introduce the concept of differential privacy and show you how it can be applied to perform privacy-preserving data analysis."
permalink: /posts/2021/03/45bc493ecffef5d89293ea2eee62ac52/
image: 2021-03-15-differential-privacy.png
canonical_url: 'https://blog.franziska-boenisch.de'
header:
  teaser: 2021-03-15-differential-privacy.png
tags:
  - differential privacy
  - privacy
---
<script src="//yihui.org/js/math-code.js"></script>
<!-- Just one possible MathJax CDN below. You may use others. -->
<script async
  src="//mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

{% include base_path %}

## *or* Federated Learning with Untrusted Servers is Not Private

*by Franziska Boenisch, Adam Dziedzic, Roei Schuster, Ali Shahin Shamsabadi, Ilia Shumailov, and Nicolas Papernot*

Every day, we all generate large amounts of data such as the text we type or the pictures we take. This data is useful: by training machine learning (ML) models on data collected from millions of individuals, a myriad of applications become possible, a prominent example of which is text auto-completion by our smartphone’s keyboard.

However, how can we collect such large amounts of data from so many users at one central server, given the communication overhead and the central storage that would be needed? And even if we could overcome the onerous communication and storage requirements, do we really *want* to transmit all this highly personal data to a central party such as our smartphone vendor? 

In a widely celebrated paper from 2017, a framework called *Federated Learning* was proposed to answer these concerns. In Federated Learning, training is performed locally on device using the individual user’s data. The products of this local training are then sent to a server, where they are aggregated from multiple devices into a global model. The server never receives the raw user data, only the products of the local training procedure (let’s call them *gradients* or *model updates*).


Since users’ data never leaves devices, FL has, for a long time, been promoted as “privacy-preserving ML”. Today --- thanks to a long line of research --- we know that model updates actually contain ample information on individuals’ data and allow full reconstruction of their data in some cases, meaning that FL cannot actually be considered privacy-preserving. However, *this did not prevent large companies from still promoting FL as privacy preserving, and deploying it to learn from millions of users’ sensitive data*. They probably did so due to two widely-adopted beliefs: first, that reconstruction methods are computationally costly, and typically obtain low-fidelity reconstructions, especially for data that is high dimensional, or containing multiple instances from the same class, or when the local gradients are calculated over many user data points. Second, that FL combines well with protocols that make reconstruction attacks significantly harder or even provably impossible, namely, (local) Differential Privacy and Secure Aggregation.

[Our new work](https://arxiv.org/pdf/2112.02918.pdf) questions both of these widely-held beliefs. Our main message is that, as of yet, FL provides no viable way to defend against a non-honest (“malicious”) server.  This means a server that intentionally deviates from the FL protocol as prescribed, in order to perform the attack. In other words, to get privacy, we still must trust the party that is responsible for FL deployment (e.g. our smartphone vendor).

We first show an attack where the central server actively manipulates the initial weights of the shared model, to directly extract user data perfectly, i.e. with zero error to the original data. Our attack is the first privacy attack in FL that relies on  manipulations of the shared model. Notably, we find that, depending on the deployment of the FL setup, our extraction procedure works even in the honest setting without  the server maliciously initializing the weights. Even in this setting, the server can exactly reconstruct user data instances in a matter of milliseconds, defeating the state-of-the-art in passive reconstruction attacks by orders of magnitude. To illustrate this, here are pictures our attacker extracted, armed with only a Python shell and Matplotlib.

<figure style="width:60%;">
    <img src="{{ "/files/test/fig1-orig-vs-extracted.png" | prepend: base_path }}"
     alt='data extraction success'/>
    <figcaption>Images from the ImageNet Dataset, extracted from the user gradients. Extraction is *perfect*, i.e., the error between the original and respective extracted images is zero. </figcaption>
</figure>


This result underscores that FL in its naive, “vanilla” form does not increase privacy when the server is untrusted. 



But what about Secure Aggregation (SA), you ask? Well, it was recently observed by [[Pasquini et al.](https://arxiv.org/pdf/2111.07380.pdf)] that SA can lose its guarantees when the server can send different models to different users. In other words, it is not secure against a malicious server, just like FL itself. Worse yet, virtually all SA  protocols rely on a proportion of participants being honest. In practical scenarios, if the party deploying the protocol (e.g. your smartphone vendor), acts maliciously, even for only short periods of time, they can invoke arbitrarily many manipulated participants (“sybils”) in the FL protocol. Thereby, they can completely circumvent the protection offered by SA. This ability of introducing manipulated participants in real-world FL systems has been shown, for example, by Google [[Ramaswamy et al.](https://arxiv.org/pdf/2009.10031.pdf)]. Under a malicious-sybil regime, it is likely that distributed DP also fails: a user cannot rely on other users’ updates being properly DP-noised, and must compensate by noising their own updates so much that learning would not likely be possible. While “hybrid” protocols that interleave elements of both SA and DP might address these issues (which we will discuss in greater depth), right now they are currently far from resolved in any practical sense.


Below, we go into the nitty-gritty details of our attack. We will first focus on passive data leakage from gradients (without malicious weight initialization), and then explain our active, malicious-server attack.

To conclude on a positive note, we will discuss some promising directions for mitigating this and other attacks, such as hybrid SA-DP and blockchain-based methods for ensuring server accountability and model cohesion across participants. With the right machinery, we believe, Federated Learning can be made reliably private in the future, and hold guarantees that the security community can understand and reason about. But we are not there yet.


# Directly Extracting Data from Model Gradients (Passive Leakage)
Let’s first have a look at the passive data leakage from model gradients. This leakage can be exploited even by an honest-but-curious central server who observes the user gradients, calculated on a fully-connected neural network layer.

## Why is One Input Data Point Perfectly Extractable from Model Gradients?

Previous work had already shown that gradients, when calculated for one single input data point $$x$$ at a fully-connected model layer contain a scaled version of this input data point. This holds if the fully-connected layer has a bias term, uses the [ReLU activation](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)), and at least for one row $$i$$ in the layer’s weight matrix, the product of the input data point and this row, plus the corresponding bias, is positive.
The setup can be visualized as follows:

<figure style="width:60%;">
    <img src="{{ "/files/test/fig2-fc-layer.png" | prepend: base_path }}"
     alt='data extraction success'/>
    <figcaption>Setup of propagating a data point $$x$$ through a fully-connected layer. </figcaption>
</figure>

