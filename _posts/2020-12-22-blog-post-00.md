---
title: 'Attacks against Machine Learning Privacy (01/04): Model Inversion Attacks with the IBM ART Framework'
date: 2020-12-22
permalink: /posts/2020/12/blog-post-0/
tags:
  - machine learning
  - privacy
  - model inversion
  - attacks
---
<script src="//yihui.org/js/math-code.js"></script>
<!-- Just one possible MathJax CDN below. You may use others. -->
<script async
  src="//mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

{% include base_path %}

Machine learning (ML) is one the fastest evolving fields out there, and it feels like every day, there are new and very useful tools emerging. 
For many scenarios, creating functional and expressive ML models might be the most important reason to use such tools. However, there is a large number of tools offering functionalities that go beyond simply building new and more powerful models, and, thereby, focussing more on different aspects of ML.

One such aspect is model privacy. The general topic of data privacy seems to receive a lot of attention also outside the tech community in the last years, especially after the introduction of legal frameworks, such as the GDPR. Yet, the topic of privacy in ML seems to be more of a niche. 
This is a pity as breaking the privacy of ML models can be as simple as that. In today's blog post I would like to show you how.

# Privacy Issues in Machine Learning
Just to briefly provide an understanding of privacy in ML, let's have a look at general ML workflows. Usually, you start with some (potentially sensible data), i.e. some data features $X$.

Imagine, we 

## Model Inversion Attacks

<figure>
    <img src="{{ "/files/2020-12-22-blog-post-00/2_1epoch.png" | prepend: base_path }}"
     alt='missing'
    style="width:30%"/>
    <figcaption>Caption goes here</figcaption>
</figure>

You have some data, potentially sensible one, you collect and clean it such that you end up with a data set, i.e. some data features `$X$`.
There exist several attacks against 

## IBM Adversarial Robustness Toolbox
Today, I would like to share .

## Using ART to Implement a Model Inversion Attack

