---
title: 'Attacks against Machine Learning Privacy (01/04): Model Inversion Attacks with the IBM-ART Framework'
date: 2020-12-22
permalink: /posts/2020/12/blog-post-0/
tags:
  - machine learning
  - privacy
  - model inversion
  - attacks
---
<script src="//yihui.org/js/math-code.js"></script>
<!-- Just one possible MathJax CDN below. You may use others. -->
<script async
  src="//mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

{% include base_path %}

Machine learning (ML) is one the fastest evolving fields out there, and it feels like every day, there are new and very useful tools emerging. 
For many scenarios, creating functional and expressive ML models might be the most important reason to use such tools. However, there is a large number of tools offering functionalities that go beyond simply building new and more powerful models, and, thereby, focussing more on different aspects of ML.

One such aspect is model privacy. The general topic of data privacy seems to receive a lot of attention also outside the tech community in the last years, especially after the introduction of legal frameworks, such as the GDPR. Yet, the topic of privacy in ML seems to be more of a niche. 
This is a pity as breaking the privacy of ML models can be as simple as that. In today's blog post I would like to show you how.
Therefore, I'll first give a short introduction about privacy risks in ML, then I'll present a specific attack, namely the *model inversion attack*, and finally I'll show you how to implement model inversion attacks with the help of [IBM's Adversarial Robustness Toolbox](https://github.com/Trusted-AI/adversarial-robustness-toolbox).

# Privacy Issues in Machine Learning
Just to briefly provide an understanding of privacy in ML, let's have a look at general ML workflows. This section only serves to give a very short and informal introduction in order to motivate the main topic of this blog post. 
For a formal and thourough introduction to ML, you may want to check out other ressources.

Imagine, you would like to train a classifier. Usually, you start with some (potentially sensible) training data, i.e. some data features $X$ and corresponding class labels $y$. You pick an algorithm, e.g. a neural network (NN), and then you use your training data to make the model learn a mapping from $X$ to $y$. This mapping should generalize well, such that your model is also able to to predict the correct labels for, so far, unseen data $X'$.

<figure style="width:30%;">
    <img src="{{ "/files/2020-12-22-blog-post-00/01_ML-workflows.png" | prepend: base_path }}"
     alt='machine learning workflow'/>
    <figcaption>A typical ML workflow.</figcaption>
</figure>

What is less frequently addressed is the fact that the process of turning training data into a good model is not necessarily one-way. 
It also makes sense: In order to learn a mapping from specific features to corresponding labels, the model needs to "remembers" information about the data it has seen in its parameters. 
Otherwise, how would it come to correct conclusions about new and unseen data?

The fact that some information about the training data is stored in the model parameters, might, however, cause privacy problems.
This is because it enables someone with access to the ML model to deduct different kinds of information about the training data.

## Model Inversion Attacks
A very popular attack is the so-called *model inversion attack* that was first proposed by [Fredrikson et al.](https://www.cs.cmu.edu/~mfredrik/papers/fjr2015ccs.pdf) in 2015.
The attack uses a trained classifier in order to extract representations of the training data.

<figure style="width:40%;">
    <img src="{{ "/files/2020-12-22-blog-post-00/01_ML-workflows-inversion.png" | prepend: base_path }}"
     alt='machine learning workflow'/>
    <figcaption>The idea of a model inversion attack.</figcaption>
</figure>

Fredrikson et al. use this method, among others, on a face classifier that was trained on black and white images of 40 different individuals' faces. 
The data features $X$ in this example correspond to the individual image pixels that can take continuous values in the range of $[0,1]$. With this large number of different pixel values combinations over an image, it is inefficient to brute-force a reconstruction over all possible images in order to identify the most likely one(s). Therefore, the authors proposed a different approach.
Given $m$ different faces and $n$ pixel values per image. The face recognition classifier can be expressed as a function $f: [0,1]^n \longmapsto [0,1]^m$. The output of the classifier is a vector that represents the probabilities of the image to belong to each of the $m$ classes. 

The authors define a cost function $c(x)$ concerning $f$ in order to do the model inversion. Starting with a candidate solution image $x_0$, the gradients of the cost function are calculated. Then,  *gradient descent* is applied,  and $x_0$ is transformed iteratively in order to minimize the cost function to find its minimum in the process. This minimum is the reconstructed image.

Let's have a concrete look at the algorithm specified in their paper with the parameters being: $label$: label of the class we want to inverse, $\alpha$: number of iterations to calculate, $\beta$: a form of patience: if the cost does not decrease within this number of iterations, the algorithm stops, $\gamma$: minimum cost threshold, $\lambda$: learning rate for the gradient descent, and $AUXTERM$: a function that uses any available auxiliary information to inform the cost function. (In the case of simple inversion, there exists no auxiliary information, i.e. $AUXTERM=0$ for all $x$.

<figure style="width:40%;">
    <img src="{{ "/files/2020-12-22-blog-post-00/01_ML-workflows-inversion.png" | prepend: base_path }}"
     alt='machine learning workflow'/>
    <figcaption>The idea of a model inversion attack.</figcaption>
</figure>

As we can see, in the algorithm, the cost function is defined based on $f$'s prediction on $x$, $x_0$ is initialized (e.g. here as a zero vector$, then for the number of epochs, the gradient descent step is performed and the new costs are calculated. There are two stopping conditions that interrupt the algorithm: (1) if the cost has not improved for the last $\beta$ epochs, and (2) if the costs are smaller than the predefined threshold $\gamma$. 

At the end of the algorithm, the minimal costs and the corresponding $x_i$ are returned. As, in the case of Fredrikson, each individual corresponds to a separate class label, the model inversion can be used in order to reconstruct concrete faces, such as the following:

<figure style="width:40%;">
    <img src="{{ "/files/2020-12-22-blog-post-00/01_ML-workflows-inversion.png" | prepend: base_path }}"
     alt='machine learning workflow'/>
    <figcaption>The idea of a model inversion attack.</figcaption>
</figure>

## IBM Adversarial Robustness Toolbox
As stated before, nowadays, there exist several great programming libraries for machine learning and the [IBM Adversarial Robustness Toolbox](https://arxiv.org/abs/1807.01069) (IBM-ART) is definitely one of them. 

Unlike what the name suggests, IBM-ART is by far not limited to functionality concerning adversarial robustness, but also contains methods for data poisoning, model extraction, and model privacy.

As most of my research is centered around model privacy, I was very keen on trying out the broad range of functionalities offered for the latter one. Next to membership inference attacks, and attribute inference attacks, the framework also offers an implementation of model inversion attacks from the Fredrikson paper. 


## Using ART to Implement a Model Inversion Attack

```python
a, b = 0, 1
while b < 10:
    print(b)
    a, b = a, a + b
  ```

<figure>
    <img src="{{ "/files/2020-12-22-blog-post-00/2_1epoch.png" | prepend: base_path }}"
     alt='missing'
    style="width:30%"/>
    <figcaption>Caption goes here</figcaption>
</figure>
