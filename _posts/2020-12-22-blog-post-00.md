---
title: 'Attacks against Machine Learning Privacy (01/04): Model Inversion Attacks with the IBM ART Framework'
date: 2020-12-22
permalink: /posts/2020/12/blog-post-0/
tags:
  - machine learning
  - privacy
  - model inversion
  - attacks
---

{% include base_path %}

Machine learning (ML) is one the fastest evolving fields out there, and it feels like every day, there are new and very useful tools emerging. 
For many scenarios, creating functional and expressive ML models might be the most important reason to use such tools. However, there is a large number of tools offering functionalities that go beyond simply building new and more powerful models, and, thereby, focussing more on different aspects of ML.

One such aspect is model privacy. The general topic of data privacy seems to receive a lot of attention also outside the tech community in the last years, especially after the introduction of legal frameworks, such as the GDPR. Yet, the topic of privacy in ML seems to be more of a niche. 
This is a pity as breaking the privacy of ML models can be as simple as that. In today's blog post I would like to show you how.

## Model Inversion Attacks
<figure>
    <img src="{{ "/files/2020-12-22-blog-post-00/2_1epoch.png" | prepend: base_path }}"
     alt='missing' />
    <figcaption>Caption goes here</figcaption>
</figure>


     
## IBM Adversarial Robustness Toolbox
