---
title: 'Attacks against Machine Learning Privacy (01/04): Model Inversion Attacks with the IBM ART Framework'
date: 2020-12-22
permalink: /posts/2020/12/blog-post-0/
tags:
  - machine learning
  - privacy
  - model inversion
  - attacks
---
<script src="//yihui.org/js/math-code.js"></script>
<!-- Just one possible MathJax CDN below. You may use others. -->
<script async
  src="//mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

{% include base_path %}

Machine learning (ML) is one the fastest evolving fields out there, and it feels like every day, there are new and very useful tools emerging. 
For many scenarios, creating functional and expressive ML models might be the most important reason to use such tools. However, there is a large number of tools offering functionalities that go beyond simply building new and more powerful models, and, thereby, focussing more on different aspects of ML.

One such aspect is model privacy. The general topic of data privacy seems to receive a lot of attention also outside the tech community in the last years, especially after the introduction of legal frameworks, such as the GDPR. Yet, the topic of privacy in ML seems to be more of a niche. 
This is a pity as breaking the privacy of ML models can be as simple as that. In today's blog post I would like to show you how.

# Privacy Issues in Machine Learning
Just to briefly provide an understanding of privacy in ML, let's have a look at general ML workflows. Imagine, you would like to train a classifier. Usually, you start with some (potentially sensible) training data, i.e. some data features $X$ and corresponding class labels $y$. You pick an algorithm, e.g. a neural network (NN), and then, you use your training data to make the model learn a mapping from $X$ to $y$. This mapping should generalize well, such that your model is also able to to predict the correct labels for, so far, unseen data $X'$.

<figure>
    <img src="{{ "/files/2020-12-22-blog-post-00/01_ML-workflows.png" | prepend: base_path }}"
     alt='machine learning workflow'
    style="width:30%"/>
    <figcaption>A typical ML workflow.</figcaption>
</figure>

What is less frequently addressed is the fact that the process of turning training data into a good model is not necessarily one-way. 
Somehow it makes sense that if we use a model to learn a mapping from specific features to corresponding labels, 

Even though you might not be able to understand how each of your training data points influences what the model learns, and even though you do not understand how your model 

```python
a, b = 0, 1
while b < 10:
    print(b)
    a, b = a, a + b
  ```

## Model Inversion Attacks

<figure>
    <img src="{{ "/files/2020-12-22-blog-post-00/2_1epoch.png" | prepend: base_path }}"
     alt='missing'
    style="width:30%"/>
    <figcaption>Caption goes here</figcaption>
</figure>

You have some data, potentially sensible one, you collect and clean it such that you end up with a data set, i.e. some data features `$X$`.
There exist several attacks against 

## IBM Adversarial Robustness Toolbox
Today, I would like to share .

## Using ART to Implement a Model Inversion Attack

