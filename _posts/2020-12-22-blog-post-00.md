---
title: 'Attacks against Machine Learning Privacy (01/04): Model Inversion Attacks with the IBM ART Framework'
date: 2020-12-22
permalink: /posts/2020/12/blog-post-0/
tags:
  - machine learning
  - privacy
  - model inversion
  - attacks
---
<script src="//yihui.org/js/math-code.js"></script>
<!-- Just one possible MathJax CDN below. You may use others. -->
<script async
  src="//mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

{% include base_path %}

Machine learning (ML) is one the fastest evolving fields out there, and it feels like every day, there are new and very useful tools emerging. 
For many scenarios, creating functional and expressive ML models might be the most important reason to use such tools. However, there is a large number of tools offering functionalities that go beyond simply building new and more powerful models, and, thereby, focussing more on different aspects of ML.

One such aspect is model privacy. The general topic of data privacy seems to receive a lot of attention also outside the tech community in the last years, especially after the introduction of legal frameworks, such as the GDPR. Yet, the topic of privacy in ML seems to be more of a niche. 
This is a pity as breaking the privacy of ML models can be as simple as that. In today's blog post I would like to show you how.

# Privacy Issues in Machine Learning
Just to briefly provide an understanding of privacy in ML, let's have a look at general ML workflows. This section only serves to give a very short and informal introduction in order to motivate the main topic of this blog post. 
For a formal and thourough introduction to ML, you may want to check out other ressources.

Imagine, you would like to train a classifier. Usually, you start with some (potentially sensible) training data, i.e. some data features $X$ and corresponding class labels $y$. You pick an algorithm, e.g. a neural network (NN), and then you use your training data to make the model learn a mapping from $X$ to $y$. This mapping should generalize well, such that your model is also able to to predict the correct labels for, so far, unseen data $X'$.

<figure style="width:30%;">
    <img src="{{ "/files/2020-12-22-blog-post-00/01_ML-workflows.png" | prepend: base_path }}"
     alt='machine learning workflow'/>
    <figcaption>A typical ML workflow.</figcaption>
</figure>

What is less frequently addressed is the fact that the process of turning training data into a good model is not necessarily one-way. 
It also makes sense: In order to learn a mapping from specific features to corresponding labels, the model needs to "remembers" information about the data it has seen in its parameters. 
Otherwise, how would it come to correct conclusions about new and unseen data?

The fact that some information about the training data is stored in the model parameters, might, however, cause privacy problems.
This is because it enables someone with access to the ML model to deduct information about the training data.

## Model Inversion Attacks


<figure style="width:40%;">
    <img src="{{ "/files/2020-12-22-blog-post-00/01_ML-workflows-inversion.png" | prepend: base_path }}"
     alt='machine learning workflow'/>
    <figcaption>The idea of a model inversion attack.</figcaption>
</figure>

<figure>
    <img src="{{ "/files/2020-12-22-blog-post-00/2_1epoch.png" | prepend: base_path }}"
     alt='missing'
    style="width:30%"/>
    <figcaption>Caption goes here</figcaption>
</figure>

You have some data, potentially sensible one, you collect and clean it such that you end up with a data set, i.e. some data features `$X$`.
There exist several attacks against 

```python
a, b = 0, 1
while b < 10:
    print(b)
    a, b = a, a + b
  ```

## IBM Adversarial Robustness Toolbox
Today, I would like to share .

## Using ART to Implement a Model Inversion Attack

